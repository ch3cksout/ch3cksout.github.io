<H1>
 Xitterspace history of GPT-chess, annotated
</H1>

<H2>
 <a href="https://nitter.net/GrantSlatton/status/1703913578036904431">
   Grant Slatton: 
 </a>  
 “the new GPT model, gpt-3.5-turbo-instruct, can play chess around 1800 Elo”
</H2>

<H3>
 <a href="https://nitter.net/GrantSlatton/status/1703913578036904431">
 The tweet remarkably lacks any supporting data,
 </a> aside from 
 <a href="https://lichess.org/K6Q0Lqda">
 a single game shown.
 </a>
</H3>

<P>
 Slatton's narrative went: "The new model readily beats Stockfish Level 4 (1700) and still loses respectably to Level 5 (2000)."
 Note that actual playing strength of Stockfish crucially depends on parameteters like hash size and thinking time,
 none of which were reported.
</P>

<P>
 It was also claimed that gpt-3.5-turbo-instruct never attempts illegal moves, which is readily shown to be false
 - it does often make illegal moves, as a matter of fact.
Chess emulators based on it typically spoil about 1 in 10 games.
  
</P>

<P>
  Here are but two examples, obtained via running
  <a href="https://github.com/clevcode/skynet-dev">
    'parrotchess' code
  </a>.
</P>
<P>
  In 
  <a href="https://www.chess.com/analysis/library/2CL6GcjFot">
    this game,
  </a> the bot attempts to jump over a rook by its queen: <B>Qg1</B>. 
  <iframe id="11138717" allowtransparency="true" frameborder="0" style="width:100%;border:none;" src="//www.chess.com/emboard?id=11138717"></iframe><script>window.addEventListener("message",e=>{e['data']&&"11138717"===e['data']['id']&&document.getElementById(`${e['data']['id']}`)&&(document.getElementById(`${e['data']['id']}`).style.height=`${e['data']['frameHeight']+30}px`)});</script>
</P>
 And  
  <a href="https://www.chess.com/analysis/library/3JXfH59qCz">
    in this one,
  </a>
 its king blithely steps into check: <B>Kd2</B>.
<iframe id="11138763" allowtransparency="true" frameborder="0" style="width:100%;border:none;" src="//www.chess.com/emboard?id=11138763"></iframe><script>window.addEventListener("message",e=>{e['data']&&"11138763"===e['data']['id']&&document.getElementById(`${e['data']['id']}`)&&(document.getElementById(`${e['data']['id']}`).style.height=`${e['data']['frameHeight']+30}px`)});</script>
</P>

<H2>
 <a href="https://nitter.net/a_karvonen/status/1705340535836221659#m">
  Adam Karvonen
 </a>  
 then followed up with a neat quantitative study.
</H2>

<H3>
 <a href="https://nitter.net/GrantSlatton/status/1703913578036904431">
 Here the full code, as well as datasets
 </a>  
 were reported, along with a nice Jupyter notebook on the statistics.
</a>
</H3>

<P>
 A shortcoming of
 <a href="https://nitter.net/a_karvonen/status/1705340535836221659#m">
  Karvonen's setup
 </a>  
 is the very short thinking time set for Stockfish: 100 ms, which severely reduces its playing strength.
 (6 s per move is already considered fast playing for the engine!)
 Also, GPT always got to play White, which makes this set of games unsuitable for realistic Elo computation.
</P>

<P>
 My quick analysis of the reported results indicated that the putative rating gained
by playing against various Stockfish levels was heavily skewed:
 performance was relatively better against weak opponents, and went sharply down against better ones.
</P>
